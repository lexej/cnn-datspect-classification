\section{Discussion}
\label{sec:discussion}


% Objective 1: Evaluation of Methods on Test set of dev data

%   Objective 1.1: To compare the "performance" of the methods on Test set of dev data.
%   Objective 1.2: To assess the suitability of these methods in unseen real-world scenarios


% Objective 2: Evaluation of Methods on PPMI and MPH dataset
%				-> understand the impact of varying dataset characteristics by evaluating on


 

%Interpretation of Results:
%Analyze the findings in the context of the research questions and objectives.

Interpretation:
- PPMI is easier to classify than test set of dev data and MPH 

%Comparison with Baseline Methods:
%%Discuss how Method 1, Method 2, and Method 3 compare with Baseline A and Baseline B.

%Identification of Patterns/Trends:
%Identify any patterns or trends in the results and discuss their significance.

%Addressing Research Questions:
%Discuss how well the research questions were answered based on the results.

%Limitations and Challenges:
%Address any limitations in the methodology or data that might have affected the results.
%Discuss challenges faced during the experiments.
Limitations: 
- bACC-AUC metric may be less human interpretable compared to the standard AUC, 
however when usefull to compare performance of classification models
- Higher variance across random splits is a weak point of the bAcc-AUC metric..


%Implications and Applications:
%Discuss the practical implications of the results in real-world ML applications.

%Future Work:
%Propose directions for future research based on the outcomes and limitations of the current study.

%Conclusion:
%Summarize the key points discussed in the evaluation and discussion chapters.