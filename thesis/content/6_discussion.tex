\section{Discussion}
\label{sec:discussion}

\subsection{Interpretation of Results}

The performance results summarized in Figure~\ref{fig:auc_comparison_methods_data} demonstrate 
that the RLT strategy leads to slightly better AUC-bACC performance (higher by $0.05-0.1\%$) on the development data test set 
and the external PPMI test dataset compared to the MVT strategy.
On the internal MPH test dataset the AUC-bACC performance of the CNN using RLT strategy is higher by $0.4\%$ compared to the MVT strategy.
Since the MPH dataset cases exhibit better spatial resolution than the development dataset and PPMI dataset cases
and thus are potentially more difficult to classify 
the clear superiority of the RLT strategy on this test dataset is particularly remarkable.
Hence the findings support the primary hypothesis of this work.

% AUC-bACC comparison of different methods and dataset
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{content/figures/evaluations/auc_main_comparison.png}
    \caption{AUC-bACC achieved by baseline and experimental methods on different test data. 
    The AUC-bACC was calculated for the mean balanced accuracy over the percentage of inconclusive cases 
    in the considered test set.} 
    \label{fig:auc_comparison_methods_data}
\end{figure} 


The CNN-based methods outperform both benchmark methods on the PPMI and MPH test datasets 
as can be seen in Figure~\ref{fig:auc_comparison_methods_data}.
The CNN-based methods consistently achieve over 2\% higher AUC-bACC results compared to the SBR benchmark method
across all test datasets.
The AUC-bACC performance of the multivariate benchmark method PCA-RFC closely approaches that of the CNN-based methods 
on the development data test set and PPMI test dataset.
Therefore, both secondary hypotheses are supported by the findings.
On the MPH test dataset, the AUC-bACC performance of the PCA-RFC method is over 3\% lower
compared to that of the CNN-based methods.
This suggests that the benchmark PCA-RFC method is particularly sensitive to varying imaging characteristics.
In general, the AUC-bACC performance on the MPH test dataset is significantly lower 
than that on the development data test set and PPMI test dataset across all classification methods.
A possible explanation for that is the higher spatial resolution of MPH dataset cases
which are harder to classify for the methods that were trained on cases with lower spatial resolution (development dataset).
On the MPH dataset, the AUC-bACC performance advantage of the CNN-based methods is more prominent 
particularly compared to the PCA-RFC method.
The higher performance on the PPMI dataset across all methods suggests that the smoothened augmented images
of the development dataset helped to generalize to the lower spatial resolution cases in the PPMI set.


\subsection{Practical Implications}

The findings of the study have practical implications for the classification of DAT-SPECT images.
First, the study shows that random label selection as a ground-truth label selection strategy 
can lead to better performance results compared to the majority vote strategy 
when training a CNN classifier for Parkinson's disease diagnosis based on DAT-SPECT.
However, considering that the random label strategy requires visual assessment of the DAT-SPECT images by several readers 
the practical benefit may be not significant enough to justify the additional assessment costs.

Second, the mean AUC-bACC of balanced accuracy on conclusive cases over the mean percentage of observed inconclusive cases (PIncObs)
can be used as a metric to decide for a concrete binary classification approach given a set of possible methods.
The metric decouples the classification model performance from the arbitrarily chosen inconclusive interval bounds.
Given a chosen classification method for practical application,
the balanced accuracy on conclusive cases over PIncObs allows to decide for the operating point (inconclusive interval)
based on the target balanced accuracy.
In practice, the target balanced accuracy can vary across applications.
For example, given a target balanced accuracy of 98\% the required PIncObs might be 2\%. 
The target PIncObs of 2\% can then be mapped back to the corresponding percentage of inconclusive cases in 
validation set (PIncVal).
The inconclusive interval associated with this PIncVal can then be used 
as an operating point for the practical application.
It should be noted that the applicability of the AUC-bACC metric is not limited to the medical field
and extends to general binary classification problems.

Third, the results once again confirm the superiority of CNN-based methods for DAT-SPECT classification compared to the
widely adopted SBR method in clinical practice, highlighting the importance of transitioning to CNN-based approaches.

\subsection{Limitations of the Study}

\subsubsection{Significance of AUC-bACC results}

There are several limitations to be considered that may impact the validity of the applied methods and results.
First, statistical significance testing was not conducted for the differences in AUC-bACC results among the methods.
Also, the main metric used to compare the model performance, the AUC-bACC of mean balanced accuracy over mean PIncObs, 
depends on a set of inconclusive intervals determined within the validation set of the development dataset 
for each classification method and randomization individually.
Since the balanced accuracy and PIncObs are averaged across the results for each random split 
the reliability of the metric may be affected by the standard deviation of both variables across the random splits.
To enhance the reliability of the metric a higher number of random splits can be used.
Also, the resolution of the balanced accuracy over PIncObs decreases as the density of test set 
predictions around the cutoff increases in comparison to the validation set predictions.
Finally the metric may be less intuitively understandable and requires more expertise when interpreting the results
when compared to standard classification metrics such as balanced accuracy and AUC-ROC.


\subsubsection{Generalizability of classification models}

The results show that AUC-bACC performance on the MPH test across all considered classification methods, 
is significantly lower than on the test set of the development dataset.
The variation in image characteristics resulting from the augmentation of the training data,
as described in Section~\ref{subsec:augment}, seems not to be sufficient to be robust 
with respect to higher spatial resolution images as contained in the MPH dataset.
Also the potential site-specific bias of the classification models can only be assessed 
on the external PPMI dataset 
since the MPH dataset originates from clinical routine at UKE, as the development dataset.


% directions for future research based on the outcomes and limitations of the current study
\subsection{Future Research}

Future research attempts should focus on testing the statistical significance of 
the differences in AUC-bACC results obtained.
Confidence intervals (CI) of the relative AUC-bACC estimates can be calculated to assess the statistical significance.
The effect of CNN hyperparameter tuning on the AUC-bACC performance of different CNN-based methods 
can be investigated in future studies.
The potential performance benefits when using volumetric DAT-SPECT images instead of 2-dimensional DVR slabs 
are to be addressed.
To better assess the generalizability of the models to external data a larger subset of the PPMI database could be used.
To enhance the robustness of classification models with respect to higher spatial resolution 
one should also include DAT-SPECT images acquired using pinhole collimators in the training set.




% How much effort is it to transfer the data into the standard stereotactic space in practice, is this a limitation, 
% is this a robust/automated process or does that involve manual QC? 

% Are you speculating why the performance on the MPH, PPMI set are quite different, 
%   reliability of ground truth, patient population, data sources?


%Proportion of borderline cases is smaller in the PPMI dataset compared to the development dataset. 
%The PPMI dataset represents a highly selected sample including research participants with a clear diagnosis of 
%PD and healthy volunteers. 
%In contrast, the development dataset comprises consecutive patients from everyday clinical routine with CUPS. 
%Thus, all subjects suffered from clinically unclear parkinsonism that did not allow a sufficiently reliable 
%symptom-based diagnosis. 
