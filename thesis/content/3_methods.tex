\section{Methods}
\label{sec:methods}

\subsection{Software Tools and Libraries}
\label{subsec:libs}

//TODO

\subsection{Development Data Preparation}

In the following, the data preparation techniques applied to the development dataset are explained in detail.

\subsubsection{Data Preprocessing}
\label{subsubsec:img_preprocess_dev}

% Preprocessing of development dataset

% TODO -> wo "as described previously" -> näher beschreiben

Individual DAT-SPECT images were stereotactically normalized to the anatomical space of the Montreal Neurological Institute (MNI) 
using the Normalize tool of the Statistical Parametric Mapping software package (version SPM12) and a set of custom DAT-SPECT templates 
representative of normal and different levels of Parkinson-typical reduction of striatal uptake as target [73]. 
The voxel size of the stereotactically normalized images was 2x2x2 mm$^{3}$. 
Intensity normalization was achieved by voxelwise scaling to the individual 75th percentile of the voxel intensity in a reference region 
comprising the whole brain without striata, thalamus, brainstem, cerebellum, and ventricles [74]. 
The resulting images are distribution volume (DVR) images. 
A 2-dimensional transversal DVR slab of 12mm thickness and 91x109 pixels with 2 mm edge length was obtained by averaging 6 transversal slices through the striatum [75]. 

\subsubsection{Data Augmentation}
\label{subsec:augment}

Data augmentation was applied to the development dataset to increase the heterogeneity of the data.
To enhance robustness across various attenuation correction and scatter correction methods, 
each image was generated in a version with and without attenuation and scatter corrections applied.
Also 3D-smoothing was employed for augmentation using an isotropic Gaussian kernel with various 
Full Width at Half Maximum (FWHM) values (FWHM = 10, 12, 14, 16, 18mm).
Thereby an augmented dataset of 20,880 images in total was constructed based on 1,740 cases.
An example of two cases augmented using the described techniques is depicted in Figure~\ref{fig:dev_dataset}.

\begin{figure}[t]
    \centering
    \colorbox{black}{%
     \includegraphics[width=0.9\textwidth]{content/figures/dev_dataset.png}%
     }
    \caption{Images obtained through augmentation of two sample cases from the development dataset, 
    a healthy case (above) and a PD case with reduced availability of DAT in the striatum (below).} 
    \label{fig:dev_dataset}
  \end{figure} 

\subsubsection{Dataset Splitting}
\label{subsec:split}

% How was dev dataset split
The augmented development dataset was split into three subsets: train set (60\%), validation set (20\%) 
and test set (20\%).
While splitting the data it was ensured that the augmented images belonging to a concrete patient were put 
only into one subset.
Thereby inter-subset data leakage was prohibited.
Ten different random splits were created to train and test each of the methods.

\subsection{Univariate benchmark: Specific Binding Ratio}
\label{subsec:sbr}

% TODO -> wo "as described previously" -> näher beschreiben

The unilateral [$^{123}$I]FP-CIT specific binding ratio (SBR) was used as a benchmark classification method.
Here, the SBR in left and right putamen was obtained by hottest voxels (HV) analysis of the stereotactically normalized 
DVR image using large unilateral putamen masks predefined in MNI space [46].
It can be calculated as 

\begin{equation}\label{eq:sbr}
  \text{HV-SBR}_{unilateral} = \frac{1}{K} \sum_{k} \hat{I}_{k, ROI} \;,
\end{equation}

where $\hat{I}_{k, ROI}$ are the \textit{normalized} voxel intensities of the $K$-hottest voxels of the unilateral ROI.
The voxel intensities of the hottest voxels are normalized to the 75th percentile of the voxel intensities 
in the reference region associated with non-specific binding [46].
The minimum of the HV-SBR values from the left and right hemispheres was used for the analysis.
An in-depth elaboration on SBR analysis can be found in [46].

The SBR-based classifier was obtained as follows.
First the SBR was calculated for each case in the training set.
Then the optimal cutoff on the SBR was determined using ROC analysis and the Youden criterion~\citep{Youden1950}.
The determined optimal cutoff was then used as the decision boundary between normal cases (NC) and Parkinson's disease (PD) 
and evaluated on the test split of the development dataset for each of the 10 random splits.
Also the determined cutoff was evaluated on the PPMI and MPH datasets described in Section~\ref{subsec:external_dataset}.

\subsection{Multivariate benchmark: PCA-enhanced Random Forest}
\label{subsec:pca_rfc}

As a further benchmark, a random forest classifier was trained on PCA-transformed features of the training set 
of the development dataset.

To be comparable with CNN-based approaches, first, a 91x91 pixel square-shaped region of interest 
was defined within the 91x109 pixel DVR slab, and each development data image (of each subset) was cropped to this region.
The square-shaped region was determined by cropping an equal number of pixels from the top and bottom 
of the DVR slab along its height dimension.

Then a PCA model with 10 principle components was initialized and fit to the training set features to obtain 
the principle components of the training set.
The determined principle components were used to transform the training set to the lower-dimensional space.
An example of the principle components of the training set for one of the random splits is depicted in Figure~\ref{fig:pca_components}.

The training data transformed by the principle components was then used to train a random forest classifier with 100 decision trees.
As hyperparameters, the Gini impurity was used to assess split quality, 
with a minimum of 2 samples required to split an internal node and 1 sample needed at a leaf node.
The trained random forest classifier was evaluated on the test split of the development dataset for each of the 10 random splits.
In addition, the trained model was tested on the PPMI and MPH datasets described in Section~\ref{subsec:external_dataset}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{content/figures/pca_components_splittrain.png}
  \caption{Principle components of the training set (development dataset) for one of the random splits.} 
  \label{fig:pca_components}
\end{figure} 

\subsection{CNN-based classification}
\label{subsec:cnn_based_classification}

% Model architecture
The models of CNN-based classifiers were based on a Residual Network \\(ResNet) architecture.
More precisely, the \textit{ResNet-18}~\citep{resnet2015} model architecture consisting of 18 layers was used as basis.
The non-pretrained weights of the ResNet-18 were used as initial weights.
The ResNet-18 architecture expects input tensors of size (3, 224, 224), 
denoting images with 3 channels and spatial dimensions of 224 by 224 pixels.
Since the development data has one color channel, the architecture was modified to expect one input channel at its 
first convolutional layer.
Also the dimensions of the last fully-connected layer of the architecture were modified to produce one output node 
in the output layer.
The modified ResNet-18 model is depicted in Figure~\ref{fig:resnet}.
To obtain a probabilistic model output the sigmoid function was applied to the output layer.

% Required preprocessing for CNN: ROI cropping and interpolation to (224, 224)
Further development data preprocessing was performed to comply with the spatial input 
dimensions required by the model architecture.
First, a 91x91 pixel square-shaped region of interest was defined within the 91x109 pixel DVR slab, 
and each development data image (of each subset) was cropped to this region.
The cropping to a square shape was performed to preserve the aspect ratio while doing the subsequent upscaling.
The square-shaped region was determined by cropping an equal number of pixels from the top and bottom 
of the DVR slab along its height dimension.
Then the square-shaped images were resized to the target image size of 224x224 pixels using bicubic interpolation.

The CNN-based approaches were trained for $20$ epochs using a batch size of $64$.
For the MVT and RLT approaches (described in Section~\ref{subsubsec:cnn_based_classification_mvt_rlt}) 
the Binary Cross Entropy (BCE) loss was employed for optimization, 
whereas for the Regression approach (described in Section~\ref{subsubsec:cnn_based_classification_regression}) 
the Mean Squared Error (MSE) loss function was used.
The Adam optimization algorithm was utilized with an initial learning rate of $0.0001$.
During the training of the model, the weights of the best epoch are saved for future evaluation.
Each CNN-based approach was trained and evaluated using identical $10$ random splits of the development data.
Additionally, the trained models were tested on the PPMI and MPH datasets described in Section~\ref{subsec:external_dataset}.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{content/figures/cnn_architecture.png}
  \caption{Overview of the architecture of CNN-based approaches.} 
  \label{fig:resnet}
\end{figure} 

\subsubsection{MVT and RLT approaches}
\label{subsubsec:cnn_based_classification_mvt_rlt}

When training a CNN using the BCE loss function, 
one has to provide the ground truth label of each instance to the optimization algorithm.
Given that each instance in the development data is labeled by three independent readers, 
a selection strategy must be determined.
The following two label selection strategies are used for training the CNNs: 
Majority Vote training (MVT) and “Random Label” training (RLT).
The labels chosen using one of the two strategies are then used, together with the model predictions, 
to compute the BCE loss.

% MVT
Majority vote training involved selecting the label that received the majority of votes from the readers as the 
ground truth label.
Since there are three available labels, 
a majority is reached when two out of the three readers agree on a particular label (e.g., the normal case (NC)).
During the model training phase, the majority vote strategy was employed to select the labels for both the training 
and validation data instances.

% RLT
In contrast to MVT, random label training involved choosing a random label from the three available options
as the ground truth label.
The seed of the random number generator (responsible for the random selection) is set only once at the start of the
algorithm and is not reset between the model training epochs.
Thereby a different label could be chosen as the ground truth label for each distinct training epoch.
Here the random label selection strategy is applied both to the training and validation data.
% majority on validation better?

\subsubsection{Regression approach}
\label{subsubsec:cnn_based_classification_regression}

The regression-based approach aimed to incorporate the uncertainty regarding the ground truth label 
into the training algorithm.
Therefore, the ground-truth label was derived from the combination of the three available labels, 
resulting in a floating-point number.
Each of the following states of certainty about the label was mapped to a distinct 
floating-point valued ground-truth label: 
\emph{all readers agree on `normal' } (ground-truth label: 0.0), 
\emph{majority of readers (two out of three) agree on `normal'} (ground-truth label: 1.0/3.0), 
\emph{majority of readers (two out of three) agree on `reduced'} (ground-truth label: 2.0/3.0) 
and \emph{all readers agree on `reduced'} (ground-truth label: 1.0).
This mapping of available labels to the ground-truth label was used for both the training 
and validation data during the model training phase.

During model training the loss was computed using the Mean Square Error loss function 
which aims to minimize the mean of the squared differences between the model predictions and the ground-truth labels.
Thereby the optimization algorithm aimed to seperate cases where no consensus was reached from those where 
consensus was achieved.


\subsection{Evaluation Methods and Metrics}
\label{subsec:determinationInconcl}

% Determination of Inconclusive Ranges on the Probabilistic Output
// TODO

On the validation set...

Determination of inconclusive ranges given a set of target percentages of inconclusive cases (0.2\% to 20.0\%, step 0.2\%).

e.g. what is the inconclusive range corresponding to 10\% of test cases being classified as inconclusive?

The determined inconclusive ranges were then used to compute the balanced accuracy on the conclusive cases.
